Great â€” now we design **RTTM ingestion & Kafka layer**, which is the **nervous system** of your RTTM.
Weâ€™ll be very precise and disciplined here, because this part is often where designs become hand-wavy.

Iâ€™ll cover, in order:

1. **Kafka topics for RTTM**
2. **Message schemas (exact payloads)**
3. **Who produces what**
4. **RTTM ingestion service design**
5. **Ordering, idempotency, and scale**
6. **Why this satisfies the RTTM requirement**

No external tools, no shortcuts.

---

# 1ï¸âƒ£ RTTM KAFKA TOPIC DESIGN

We do **NOT** create one topic per metric.
We create **semantic topics**, aligned with the data tables you already designed.

### âœ… Final RTTM topics

| Topic Name           | Purpose                    |
| -------------------- | -------------------------- |
| `rttm.trade.events`  | Trade lifecycle & movement |
| `rttm.queue.metrics` | Queue lag snapshots        |
| `rttm.dlq.events`    | DLQ routing                |
| `rttm.error.events`  | Errors & failures          |
| `rttm.log.events`    | Parsed/interpreted logs    |

ğŸ“Œ These topics are **internal-only**
ğŸ“Œ Retention can be short (Kafka is not the source of truth â€” DB is)

---

# 2ï¸âƒ£ MESSAGE SCHEMAS (EXACT CONTRACTS)

This is **non-negotiable** â€” all services must follow these schemas.

---

## 2.1 `rttm.trade.events` (MOST IMPORTANT)

### Key

```
key = trade_id
```

### Value (JSON / Avro / Protobuf â€” conceptually)

```json
{
  "trade_id": "T123",
  "service_name": "validation-service",
  "event_type": "VALIDATION_SUCCESS",
  "event_status": "SUCCESS",
  "source_queue": "trade-input",
  "target_queue": "trade-valid",
  "topic_name": "trade-input",
  "partition": 2,
  "offset": 102345,
  "event_time": "2025-12-17T11:05:49Z",
  "message": "Trade validated successfully"
}
```

### Why this matters

* Enables **trade traceability**
* Enables **TPS**
* Enables **stuck trade detection**

---

## 2.2 `rttm.queue.metrics`

### Key

```
key = topic_name + partition
```

### Value

```json
{
  "service_name": "validation-service",
  "topic_name": "trade-input",
  "partition": 2,
  "produced_offset": 105000,
  "consumed_offset": 102345,
  "snapshot_time": "2025-12-17T11:05:50Z"
}
```

ğŸ“Œ Produced periodically (e.g., every 10s)

---

## 2.3 `rttm.dlq.events`

### Key

```
key = trade_id
```

### Value

```json
{
  "trade_id": "T124",
  "service_name": "validation-service",
  "topic_name": "trade-dlq",
  "reason": "Schema validation failed",
  "event_time": "2025-12-17T11:05:52Z"
}
```

---

## 2.4 `rttm.error.events`

### Key

```
key = service_name
```

### Value

```json
{
  "trade_id": "T125",
  "service_name": "pms-core",
  "error_type": "DB_TIMEOUT",
  "error_message": "Timeout while updating position",
  "event_time": "2025-12-17T11:05:53Z"
}
```

---

## 2.5 `rttm.log.events`

### Key

```
key = service_name
```

### Value

```json
{
  "service_name": "analytics-service",
  "log_level": "WARN",
  "log_category": "PROCESSING_DELAY",
  "trade_id": "T126",
  "message": "Analytics computation exceeded SLA",
  "event_time": "2025-12-17T11:05:54Z"
}
```

---

# 3ï¸âƒ£ WHO PRODUCES WHAT (CLEAR RESPONSIBILITY)

| Service       | Topics it Produces                                                                |
| ------------- | --------------------------------------------------------------------------------- |
| Simulation    | `rttm.trade.events`                                                               |
| Trade Capture | `rttm.trade.events`, `rttm.error.events`                                          |
| Validation    | `rttm.trade.events`, `rttm.queue.metrics`, `rttm.dlq.events`, `rttm.error.events` |
| PMS Core      | `rttm.trade.events`, `rttm.queue.metrics`, `rttm.error.events`                    |
| Analytics     | `rttm.trade.events`, `rttm.queue.metrics`, `rttm.log.events`                      |
| Leaderboard   | `rttm.trade.events`                                                               |

ğŸ“Œ **Every Kafka consumer service produces queue metrics**

---

# 4ï¸âƒ£ RTTM INGESTION SERVICE (CRITICAL COMPONENT)

### Name

**RTTM Ingestion Service**

### Responsibilities

* Consume all `rttm.*` topics
* Validate schema
* Persist into RTTM tables
* Ensure idempotency
* No business logic

---

## 4.1 Consumer groups

| Topic                | Consumer Group        |
| -------------------- | --------------------- |
| `rttm.trade.events`  | `rttm-trade-consumer` |
| `rttm.queue.metrics` | `rttm-queue-consumer` |
| `rttm.dlq.events`    | `rttm-dlq-consumer`   |
| `rttm.error.events`  | `rttm-error-consumer` |
| `rttm.log.events`    | `rttm-log-consumer`   |

ğŸ“Œ Allows independent scaling

---

## 4.2 Mapping â†’ DB (exact)

| Kafka Topic          | DB Table             |
| -------------------- | -------------------- |
| `rttm.trade.events`  | `rttm_trade_events`  |
| `rttm.queue.metrics` | `rttm_queue_metrics` |
| `rttm.dlq.events`    | `rttm_dlq_events`    |
| `rttm.error.events`  | `rttm_error_events`  |
| `rttm.log.events`    | `rttm_parsed_logs`   |

---

# 5ï¸âƒ£ ORDERING, IDEMPOTENCY & SCALE (VERY IMPORTANT)

### Ordering

* Partition key = `trade_id`
* Guarantees **in-order trade events**

### Idempotency

Each RTTM message contains:

```
(event_id OR topic + partition + offset)
```

RTTM ingestion enforces:

* Unique constraint
* Ignore duplicates (at-least-once safe)

### Scale

* RTTM ingestion can scale horizontally
* DB is append-only (writes are fast)

---

# 6ï¸âƒ£ RETENTION & FAILURE STRATEGY

### Kafka retention

* `rttm.*` topics: **1â€“3 days**
* DB is the permanent store

### Failure handling

* If RTTM is down â†’ Kafka buffers
* If DB is down â†’ consumer retries
* No back-pressure on business services

---

# 7ï¸âƒ£ WHY THIS DESIGN IS STRONG (INTERVIEW-READY)

### One-liner

> â€œWe use Kafka as a transient ingestion layer for RTTM events, while PostgreSQL remains the source of truth for observability data.â€

### Why evaluators like this

* Clean separation
* Fault-tolerant
* Replayable
* No dependency on external monitoring tools

---

# 8ï¸âƒ£ WHATâ€™S NEXT (LOGICAL ORDER)

Next we should design:

ğŸ‘‰ **RTTM Metrics Computation Engine**

* How TPS, lag, DLQ depth are computed
* Scheduling
* Windowing
* Alert evaluation

Say **â€œnext: metrics computation & alerts engineâ€** and we continue.
